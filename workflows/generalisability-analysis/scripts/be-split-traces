#!/usr/bin/env python3
"""Usage: be-split-traces [options] INFILE [OUTFILE]

Split the traces in INFILE into training, testing, and validation
datasets for the background experiment.  This would normally be done at
experiment time, however VarCNN requires the same exact dataset for the
time and sizes variants so that they can be combined later. This
approach ensures they are identical.

OUTFILE is a json stream with each line containing a dict with the keys
"train", "test", "val", and "train-val" the latter which contains a
combined permutationof "train" and "val".

Options:
    --n-splits k
        Split the data into k folds [default: 10]

    --n-repeats n
        Repeat the k-folds n times [default: 2]

    --validation-split frac
        Set aside frac fraction of the training set for validation
        [default: 0.1].

    --with-quic
        Include QUIC samples in the unmonitored test set.

    --with-monitored-quic
        Include QUIC samples in the monitored test set.

    --quic-frac frac
        The fraction of QUIC traces to be used [default: 0.5].

    --seed value
        Seed the random number generation with value [default: 16248].
"""
import json
import time
import logging
from collections import namedtuple
from typing import Iterator, IO

import h5py
import doceasy
import pandas as pd
import numpy as np
from numpy.random import RandomState
import sklearn
from sklearn.model_selection import (
    GroupShuffleSplit, RepeatedStratifiedKFold, train_test_split
)

Split = namedtuple("Split", ["train", "val", "test"])
_LOGGER = logging.getLogger("be-split-traces")


def decode_column(
    column: pd.Series, names=("url", "region", "protocol"),
    encoding: str = "ascii"
):
    """Decode the column if it has one of the names in column_names.
    """
    if column.name in names:
        return column.str.decode(encoding)
    return column


def _sample_quic(frame, quic_frac: float, random_state):
    assert "protocol" in frame

    assert frame["protocol"].nunique() == 1
    is_quic_frame = (frame["protocol"] != "tcp").all()

    if quic_frac == 0.0 and is_quic_frame:
        return None
    if quic_frac == 0.0 and not is_quic_frame:
        return frame
    if quic_frac == 1.0 and is_quic_frame:
        return frame
    if quic_frac == 1.0 and not is_quic_frame:
        return None

    frac = quic_frac if is_quic_frame else (1 - quic_frac)

    return frame.sample(frac=frac, random_state=random_state)


class Splitter:
    """Split a dataset in train and test instances.

    Monitored samples will be split in a straight-forward fashion as
    they should only comprise of TCP.  The train split of unmonitored
    samples are only TCP, the test split will either contain only TCP
    if with_quic is False, or TCP and QUIC if with_quic is True.
    """
    def __init__(
        self, n_splits: int, n_repeats: int, validation_split: float,
        with_quic: bool, random_state: np.random.RandomState,
        with_monitored_quic: bool, quic_frac: float
    ):
        self.n_splits = n_splits
        self.n_repeats = n_repeats
        self.validation_split = validation_split
        self.with_quic = with_quic
        self.with_monitored_quic = with_monitored_quic
        self.quic_frac = quic_frac
        self.random_state = sklearn.utils.check_random_state(random_state)

    def split_monitored(self, X, labels) -> Iterator[Split]:
        """Yield indicies of the monitored samples to be used for
        training, validation, and testing
        """
        mask = (labels["class"] != -1)
        X = X[mask]
        y = labels["class"][mask].values
        stratify = labels[mask][["class", "protocol"]].copy()
        stratify["protocol"] = stratify["protocol"].astype("category").cat.codes
        # Encode as 1d values
        stratify = np.unique(stratify.values, axis=0, return_inverse=True)[1]
        indices = np.arange(len(labels))[mask]

        tcp_idx = np.nonzero((labels[mask]["protocol"] == "tcp").values)[0]

        splitter = RepeatedStratifiedKFold(
            n_splits=self.n_splits, n_repeats=self.n_repeats,
            random_state=self.random_state)
        for train_val_idx, test_idx in splitter.split(X, stratify):
            # Remove all non-TCP samples from training and validation
            train_val_idx = np.intersect1d(train_val_idx, tcp_idx)

            if self.validation_split > 0:
                train_idx, val_idx = train_test_split(
                    train_val_idx, test_size=self.validation_split,
                    random_state=self.random_state, stratify=y[train_val_idx])
            else:
                train_idx, val_idx = train_val_idx, np.array([], dtype=int)

            if not self.with_monitored_quic:
                test_idx = np.intersect1d(test_idx, tcp_idx)
            else:
                # Maintain the same number of test indices as in the TCP only
                # case by downsampling the test indices
                to_sample = labels[mask].iloc[test_idx].copy()
                to_sample["mapped_index"] = test_idx
                sampled = to_sample.groupby(["class", "protocol"]).apply(
                    _sample_quic, quic_frac=self.quic_frac,
                    random_state=self.random_state)
                test_idx = sampled["mapped_index"].values

                # Shuffle the indicies as they will be grouped by protocol
                self.random_state.shuffle(test_idx)

            yield Split(indices[train_idx], indices[val_idx], indices[test_idx])

    def split_unmonitored(self, X, labels) -> Iterator[Split]:
        """Split the unmonitored set.

        Training, validation, and testing will all have unique original
        URLs.  Furthermore, the training and validation sets will only
        have TCP samples.

        The testing set will have a mixture of TCP and QUIC or solely
        TCP samples.
        """
        mask = (labels["class"] == -1)
        # Keep a reference back to the original indices, as we're using
        # masked data
        original_indices = np.arange(len(labels))[mask]

        groups = labels["group"].values

        # Remap the indices
        for train_idx, val_idx, test_idx in self._split_unmonitored(
            X[mask], labels[mask]
        ):
            train_idx = original_indices[train_idx]
            val_idx = original_indices[val_idx]
            test_idx = original_indices[test_idx]

            # Ensure that the groups do not overlap
            assert len(np.intersect1d(
                np.unique(groups[train_idx]), np.unique(groups[val_idx]))) == 0
            assert len(np.intersect1d(
                np.unique(groups[train_idx]), np.unique(groups[test_idx]))) == 0
            assert len(np.intersect1d(
                np.unique(groups[val_idx]), np.unique(groups[test_idx]))) == 0

            # Ensure that only TCP is in the train set
            assert labels["protocol"].iloc[train_idx].nunique() == 1
            assert "tcp" in labels["protocol"].iloc[train_idx].values

            # Ensure that, if present, only TCP is in the validation idx
            if self.validation_split > 0:
                assert labels["protocol"].iloc[val_idx].nunique() == 1
                assert "tcp" in labels["protocol"].iloc[val_idx].values
            else:
                assert len(val_idx) == 0

            # Ensure that the test indices contain TCP only or a mixture
            if self.with_quic:
                assert labels["protocol"].iloc[test_idx].nunique() > 1
            else:
                assert labels["protocol"].iloc[test_idx].nunique() == 1
                assert "tcp" in labels["protocol"].iloc[test_idx].values

            yield Split(train_idx, val_idx, test_idx)

    def _split_unmonitored(self, X, labels) -> Iterator[Split]:
        # Get the total number of splits since we use a shuffle split
        n_splits = self.n_splits * self.n_repeats
        # Convert a "fold" to a ratio for the test set
        test_size = 1 / self.n_splits

        groups = labels["group"].values
        tcp_idx = np.nonzero((labels["protocol"] == "tcp").values)[0]

        splitter = GroupShuffleSplit(n_splits=n_splits, test_size=test_size,
                                     random_state=self.random_state)
        for train_val_idx, test_idx in splitter.split(X, groups=groups):
            # The train indices should be all TCP
            train_val_idx = np.intersect1d(train_val_idx, tcp_idx)

            if self.validation_split > 0:
                # pylint: disable=stop-iteration-return
                train_idx, val_idx = next(GroupShuffleSplit(
                    n_splits=1, test_size=self.validation_split,
                    random_state=self.random_state
                ).split(train_val_idx, groups=groups[train_val_idx]))

                # Get the real indices, the above indices are indices into the
                # train_val_idx
                train_idx = train_val_idx[train_idx]
                val_idx = train_val_idx[val_idx]
            else:
                train_idx, val_idx = train_val_idx, np.array([], dtype=int)

            if self.with_quic:
                # Assign some groups to QUIC and some to TCP
                # pylint: disable=stop-iteration-return
                tcp_groups_idx, quic_groups_idx = next(GroupShuffleSplit(
                    n_splits=1, test_size=self.quic_frac,
                    random_state=self.random_state
                ).split(test_idx, groups=groups[test_idx]))
                # Actually restrict the tpc and quic groups to only TCP and QUIC
                test_idx = np.concatenate((
                    np.intersect1d(test_idx[tcp_groups_idx], tcp_idx),
                    np.setdiff1d(test_idx[quic_groups_idx], tcp_idx)
                ))
            else:
                test_idx = np.intersect1d(test_idx, tcp_idx)

            yield Split(train_idx, val_idx, test_idx)

    def split(self, X: np.ndarray, labels: pd.DataFrame) -> Iterator[Split]:
        """Split the dataset into train, validation, and test indices,
        ensuring that each class is represented in all three sets.

        Labels should contain groups for the unmonitored set,
        unmonitored classes labelled as -1 and which protocol each
        is associated with.

        Each URL in the unmonitored set should have at least 1 sample
        for each of QUIC and TCP.
        """
        for monitored_indices, unmonitored_indices in zip(
            self.split_monitored(X, labels), self.split_unmonitored(X, labels)
        ):
            train_idx = np.concatenate((
                monitored_indices.train, unmonitored_indices.train))
            self.random_state.shuffle(train_idx)

            val_idx = np.concatenate((
                monitored_indices.val, unmonitored_indices.val))
            self.random_state.shuffle(val_idx)

            test_idx = np.concatenate((
                monitored_indices.test, unmonitored_indices.test))
            self.random_state.shuffle(test_idx)

            yield Split(train_idx, val_idx, test_idx)


def main(infile: str, outfile: IO, **splitter_kw):
    """Script entry point."""
    logging.basicConfig(
        format='[%(asctime)s] [%(levelname)s] %(name)s - %(message)s',
        level=logging.INFO)

    start = time.perf_counter()

    with h5py.File(infile, mode="r") as h5in:
        _LOGGER.info("Reading labels from %r...", infile)
        labels = (pd.DataFrame.from_records(np.asarray(h5in["labels"]))
                  .transform(decode_column))

    _LOGGER.info("Creating splitter with args %s", splitter_kw)
    random_state = RandomState(splitter_kw.pop("seed"))
    splitter = Splitter(**splitter_kw, random_state=random_state)  # type:ignore

    X = np.zeros((len(labels), 1))
    for indices in splitter.split(X, labels):
        permutation = random_state.permutation(
            len(indices.train) + len(indices.val))

        json.dump({
            "train": indices.train.tolist(),
            "val": indices.val.tolist(),
            "test": indices.test.tolist(),
            "train-val": np.concatenate(
                (indices.train, indices.val))[permutation].tolist()
        }, outfile, indent=None, separators=(",", ":"))
        outfile.write("\n")

    _LOGGER.info("Script complete in %.2fs", (time.perf_counter() - start))


if __name__ == "__main__":
    main(**doceasy.doceasy(__doc__, {
        "INFILE": str,
        "OUTFILE": doceasy.File(mode="w", default="-"),
        "--n-splits": doceasy.Use(int),
        "--n-repeats": doceasy.Use(int),
        "--validation-split": doceasy.Use(float),
        "--quic-frac": doceasy.Use(float),
        "--seed": doceasy.Use(int),
        "--with-quic": bool,
        "--with-monitored-quic": bool,
    }))
