#!/usr/bin/env python3
"""Usage: select-traces [options] MON_FILE UNMON_FILE OUTFILE

Select monitored traces from the HDF MON_INFILE which meet the required
number of samples per region and QUIC protocol (--n-traces).  Select
unmonitored traces from the HDF UNMON_FILE which have at least 1 sample
per region for the QUIC protocol, and a multiple as many TCP samples
per region.

All the sizes are written out /sizes and timestamps to /timestamps.
Classes and metadata are written to /labels. The URLs associated with
the monitored set are encoded from 0 to n_classes, and the unmonitored
set are encoded with the class -1.  Classes associated with the URL
of the unmonitored sample can be read from the "groups" column of
/labels, and are strictly negative.

Options:
    --quic-version ver
        Output TCP along with QUIC version ver samples [default: quic]

    --n-classes count
        Return count monitored classes (urls) in the dataset, encoded
        as integers [default: 100].

    --n-traces n
        Require n QUIC traces per URL for monitored classes
        [default: 100].

    --tcp-ratio ratio
        Require ratio*n_traces TCP traces per URL for monitored classes
        [default: 1.0]

    --seed value
        Use value as the seed for pseudo-random numbers generated in
        this script.

"""
import time
import math
import pathlib
import logging
from typing import Optional

import h5py
import doceasy
import numpy as np
import pandas as pd

_LOGGER = logging.getLogger(pathlib.Path(__file__).name)
N_TRACES_PER_REGION_UNMONITORED = 1


def _urls_with_enough_per_region(traces, n_per_region: int):
    region_counts = (traces.groupby(["url", "region"]).size()
                     .unstack(level="region"))
    mask = (region_counts >= n_per_region).all(axis=1)
    return region_counts[mask].index.values


def _sample(traces, n_traces: int, random_state):
    n_regions = traces["region"].nunique()
    n_per_region = math.ceil(n_traces / n_regions)

    return (traces.groupby(["url", "region"], group_keys=False)
            .apply(pd.DataFrame.sample, n=n_per_region,
                   random_state=random_state)
            .groupby("url", group_keys=False)
            .apply(pd.DataFrame.sample, n=n_traces,
                   random_state=random_state))


def _select_traces(
    traces, n_traces: int, n_classes: int, rand, tcp_ratio: float
):
    """Speicfying a non-positive n_classes will skip checking the
    minimum number of classes.

    n_traces of -1 means 1 sample per region.
    n_classes of -1 returns all selected classes.
    """
    assert traces["protocol"].nunique() == 2
    assert "tcp" in traces["protocol"].values

    n_regions = traces["region"].nunique()
    n_traces = n_traces if n_traces > 0 else n_regions
    n_tcp_traces = int(n_traces * tcp_ratio)

    quic_urls = _urls_with_enough_per_region(
        traces[traces["protocol"] != "tcp"], math.ceil(n_traces / n_regions))
    tcp_urls = _urls_with_enough_per_region(
        traces[traces["protocol"] == "tcp"],
        math.ceil(n_tcp_traces / n_regions))
    sufficient_urls = np.intersect1d(quic_urls, tcp_urls)

    if n_classes > 0:
        if len(sufficient_urls) < n_classes:
            raise ValueError(
                f"Not enough classes after filtering to URLs with {n_traces} "
                f"traces: #QUIC={len(quic_urls)}, #tcp={len(tcp_urls)}, "
                f"#intersection={len(sufficient_urls)}")

        # Downsample to request number of URLs
        sufficient_urls = rand.choice(sufficient_urls, n_classes, replace=False)
        assert len(sufficient_urls) == n_classes

    traces = traces[traces["url"].isin(sufficient_urls)]

    traces = pd.concat([
        _sample(traces[traces["protocol"] != "tcp"], n_traces, rand),
        _sample(traces[traces["protocol"] == "tcp"], n_tcp_traces, rand)
    ]).sort_index()

    # Encode the URLs as int classes
    traces["class"] = traces["url"].astype("category").cat.codes

    return traces


def _read_from_hdf(hdf_file, quic_version: str):
    labels = (pd.DataFrame.from_records(hdf_file["/labels"][:])
              .transform(lambda col: col.str.decode("ascii"), axis=0))
    return labels[labels["protocol"].isin(("tcp", quic_version))]


def _write_selected(traces, in_hdf, out_hdf):
    dtypes = {name: col.astype("S").dtype
              for name, col in traces[["protocol", "region"]].items()}

    labels = traces[["class", "group", "protocol", "region"]].to_records(
        index=False, column_dtypes=dtypes)

    if "labels" not in out_hdf.keys():
        _LOGGER.info("Writing %d labels to a new dataset...", len(traces))
        out_hdf.create_dataset("labels", data=labels, compression="gzip",
                               maxshape=(None, ))
    else:
        _LOGGER.info("Appending %d labels to the existing dataset...",
                     len(traces))
        dataset = out_hdf["labels"]
        dataset.resize((dataset.shape[0] + len(labels), ))
        dataset[-len(labels):] = labels

    index = traces.index.values
    for key in ("sizes", "timestamps"):
        if key not in out_hdf.keys():
            _LOGGER.info("Copying %d %s to a new dataset...", len(index), key)
            out_hdf.create_dataset(f"{key}", data=in_hdf[key][index],
                                   compression="gzip", maxshape=(None, ))
        else:
            _LOGGER.info("Appending %d %s to the existing dataset...",
                         len(index), key)
            dataset = out_hdf[key]
            dataset.resize((dataset.shape[0] + len(index), ))
            dataset[-len(index):] = in_hdf[key][index]


def select_traces(
    mon_file: str, unmon_file: str, outfile: str, quic_version: str,
    n_traces: int, n_classes: int, seed: Optional[int], tcp_ratio: float
):
    """Select the traces that meet the required number of monitored and
    unmonitored traces.
    """
    logging.basicConfig(
        format='[%(asctime)s] [%(levelname)s] %(name)s - %(message)s',
        level=logging.INFO)

    start = time.perf_counter()
    rand = np.random.RandomState(seed)

    with h5py.File(outfile, mode="w") as out_hdf:
        _LOGGER.info("Openned %r for writing results.", outfile)

        with h5py.File(mon_file, mode="r") as hdf_file:
            _LOGGER.info("Reading monitored samples...")
            traces = _read_from_hdf(hdf_file, quic_version)

            _LOGGER.info("Selecting traces from %d samples...", len(traces))
            traces = _select_traces(
                traces, n_traces=n_traces, n_classes=n_classes, rand=rand,
                tcp_ratio=tcp_ratio)
            traces["group"] = 0

            _LOGGER.info("Writing %d selected samples...", len(traces))
            _write_selected(traces, hdf_file, out_hdf)

        with h5py.File(unmon_file, mode="r") as hdf_file:
            _LOGGER.info("Reading unmonitored samples...")
            traces = _read_from_hdf(hdf_file, quic_version)

            _LOGGER.info("Selecting traces from %d samples...", len(traces))
            traces = _select_traces(
                traces, n_traces=-1, n_classes=-1, rand=rand,
                tcp_ratio=tcp_ratio)
            traces["group"] = -1 * (traces["class"] + 1)
            traces["class"] = -1

            _LOGGER.info("Writing %d selected samples...", len(traces))
            _write_selected(traces, hdf_file, out_hdf)

    _LOGGER.info("Script complete in %.2fs.", (time.perf_counter() - start))


if __name__ == "__main__":
    select_traces(**doceasy.doceasy(__doc__, {
        "MON_FILE": str,
        "UNMON_FILE": str,
        "OUTFILE": str,
        "--quic-version": str,
        "--n-classes": doceasy.Use(int),
        "--n-traces": doceasy.Use(int),
        "--tcp-ratio": doceasy.Use(float),
        "--seed": doceasy.Or(None, doceasy.Use(int)),
    }))
